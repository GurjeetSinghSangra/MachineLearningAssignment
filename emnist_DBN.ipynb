{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emnist_DBN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOL7+dT4i5QTt30OODuDxth",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GurjeetSinghSangra/MachineLearningAssignment/blob/main/emnist_DBN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUbP8YY3Fwke",
        "outputId": "24ad08a6-4f42-485f-b6d5-57a8446717d2"
      },
      "source": [
        "##################################################\n",
        "# Imports\n",
        "##################################################\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn #torch made by layers, in nn we have linear layer which implements standard forward fully connected layer\n",
        "import torch.nn.functional as F \n",
        "import torchvision #used for data loading\n",
        "import torch.optim as optim #optimizer for the gradient\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "\n",
        "# Setting the device\n",
        "if torch.cuda.is_available(): \n",
        "    print('GPU enabled!')\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    print('You are not using the GPU, activate it following:')\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You are not using the GPU, activate it following:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGQEKA80CLxC"
      },
      "source": [
        "def rotate(image):\n",
        "    image = image.reshape([28, 28])\n",
        "    image = np.fliplr(image)\n",
        "    image = np.rot90(image)\n",
        "    return image\n",
        "    \n",
        "images = np.apply_along_axis(rotate, 1, x_train)\n",
        "plt.imshow(images[1,], cmap=plt.get_cmap('gray')) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7Q0OXOsKDcF"
      },
      "source": [
        "class DBN(nn.Module):\n",
        "  DN = {}\n",
        "  DN['layersize'] = np.array([50, 50, 200])\n",
        "  DN['maxepochs'] = 10\n",
        "  DN['batchsize'] = 500\n",
        "\n",
        "  def __init__(self, layers_nn, maxepochs=20, batchsize=500, epsilonw=0.1, \\\n",
        "                               sparsity=1, spars_factor=0.04, epsilonvb=0.1, epsilonhb=0.1, weightcost=0.0002, init_momentum=0.5, final_momentum=0.9):\n",
        "    super(DBN, self).__init__()\n",
        "    self.DN['layersize'] = layers_nn\n",
        "    self.DN['nlayers'] = len(layers_nn)\n",
        "    self.DN['batchsize'] = batchsize\n",
        "    self.DN['maxepochs'] = maxepochs\n",
        "    self.epsilonw_GPU = epsilonw\n",
        "    self.epsilonvb=epsilonvb\n",
        "    self.epsilonhb=epsilonhb\n",
        "    self.weightcost = weightcost\n",
        "    self.initialmomentum = init_momentum\n",
        "    self.finalmomentum = final_momentum\n",
        "    self.sparsity = sparsity\n",
        "    self.spars_factor = spars_factor\n",
        "    if self.DN['maxepochs']  > 5:\n",
        "      self.momentum = self.finalmomentum\n",
        "    else:\n",
        "      self.momentum = self.initialmomentum\n",
        "\n",
        "  def load_data(self, fname):\n",
        "    dataset = scipy.io.loadmat(fname)\n",
        "    data_train = dataset['dataset']['train']\n",
        "    data_test = dataset['dataset']['test']\n",
        "    x_train = data_train[0,0]['images'][0,0]\n",
        "    x_train = x_train.astype('float32') / 255\n",
        "    y_train = data_train[0,0]['labels'][0,0]\n",
        "    x_test = data_test[0,0]['images'][0,0]\n",
        "    x_test = np.double(x_test) / 255\n",
        "    y_test = data_test[0,0]['labels'][0,0]\n",
        "\n",
        "    batchsize = self.DN['batchsize']\n",
        "    iterations = int(len(x_train)/batchsize)\n",
        "    batchdata = np.zeros(shape=(batchsize,784,iterations), dtype='float32')\n",
        "    for i in range(iterations) :\n",
        "        for j in range(batchsize):\n",
        "            batchdata[j, :,i] = x_train[i+j, :]\n",
        "\n",
        "    tensor_x = torch.Tensor(batchdata)\n",
        "    tensor_y = torch.Tensor(y_train)\n",
        "    x_trainGPU = torch.from_numpy(batchdata)\n",
        "    return (x_train, y_train, x_test, y_test, x_trainGPU)\n",
        "  \n",
        "  def train_dbn(self, batchdata): \n",
        "    for layer in range(0, self.DN['nlayers']):\n",
        "\n",
        "        print ('Training layer ', layer+1, '...')\n",
        "        if layer == 0:\n",
        "            data_GPU = batchdata\n",
        "        else:\n",
        "            data_GPU = batchposhidprobs\n",
        "\n",
        "        numhid = self.DN['layersize'][layer]\n",
        "        numcases, numdims, numbatches = data_GPU.shape\n",
        "        vishid_GPU       = 0.1 * torch.randn(numdims, numhid)\n",
        "        hidbiases_GPU    = torch.zeros(numhid)\n",
        "        visbiases_GPU    = torch.zeros(numdims)\n",
        "        vishidinc_GPU    = torch.zeros((numdims, numhid))\n",
        "        hidbiasinc_GPU   = torch.zeros(numhid)\n",
        "        visbiasinc_GPU   = torch.zeros(numdims)\n",
        "        batchposhidprobs = torch.zeros((numcases, numhid, numbatches))\n",
        "\n",
        "\n",
        "        sigmoid = nn.Sigmoid()\n",
        "        for epoch in range(self.DN['maxepochs']):\n",
        "            for mb in range(numbatches):\n",
        "                data_mb = data_GPU[:, :, mb]\n",
        "\n",
        "                #%%%%%%%% START POSITIVE PHASE %%%%%%%%%\n",
        "                #print(data_mb.shape)\n",
        "                #print(vishid_GPU.shape)\n",
        "                #1./(1 + exp(-poshidstates * vishid' - repmat(visbiases, numcases, 1))); \n",
        "                poshidprobs_GPU = sigmoid((torch.mm(data_mb, vishid_GPU) + hidbiases_GPU))\n",
        "                posprods_GPU    = torch.mm(data_mb.T, poshidprobs_GPU)\n",
        "                poshidact_GPU   = poshidprobs_GPU.sum(0)\n",
        "                posvisact_GPU   = data_GPU[:, :, mb].sum(0)\n",
        "                #%%%%%%%% END OF POSITIVE PHASE %%%%%%%%%\n",
        "                poshidstates_GPU = poshidprobs_GPU > poshidprobs_GPU.rand()\n",
        "\n",
        "                #%%%%%%%% START NEGATIVE PHASE  %%%%%%%%%\n",
        "                negdata_GPU     = sigmoid((torch.mm(poshidstates_GPU, vishid_GPU.T) + visbiases_GPU))\n",
        "                neghidprobs_GPU = sigmoid((torch.mm(negdata_GPU, vishid_GPU) + hidbiases_GPU))\n",
        "                negprods_GPU    = torch.mm(negdata_GPU.T, neghidprobs_GPU)\n",
        "                neghidact_GPU   = neghidprobs_GPU.sum(0)\n",
        "                negvisact_GPU   = negdata_GPU.sum(0)\n",
        "                #%%%%%%%% END OF NEGATIVE PHASE %%%%%%%%%\n",
        "\n",
        "                #%%%%%%%% UPDATE WEIGHTS AND BIASES %%%%%%%%%\n",
        "                vishidinc_GPU  = vishidinc_GPU  * self.momentum + ((posprods_GPU - negprods_GPU) / numcases - self.weightcost * vishid_GPU) * epsilonw_GPU\n",
        "                visbiasinc_GPU = visbiasinc_GPU * self.momentum + (posvisact_GPU - negvisact_GPU) * (self.epsilonvb / numcases)\n",
        "                hidbiasinc_GPU = hidbiasinc_GPU * self.momentum + (poshidact_GPU - neghidact_GPU) * (self.epsilonhb / numcases)\n",
        "                vishid_GPU     = vishid_GPU + vishidinc_GPU\n",
        "                visbiases_GPU  = visbiases_GPU + visbiasinc_GPU\n",
        "                hidbiases_GPU  = hidbiases_GPU + hidbiasinc_GPU\n",
        "                #%%%%%%%% END OF UPDATES %%%%%%%%%\n",
        "\n",
        "                if epoch == self.DN['maxepochs']-1:\n",
        "                    batchposhidprobs[:, :, mb] = poshidprobs_GPU\n",
        "\n",
        "                #TODO: Sparsity\n",
        "\n",
        "        # save learned weights\n",
        "        self.DN['vis_bias' + str(layer)] = visbiases_GPU.as_numpy_array()\n",
        "        self.DN['hid_bias' + str(layer)] = hidbiases_GPU.as_numpy_array()\n",
        "        self.DN['vishid'   + str(layer)] = vishid_GPU.as_numpy_array()\n",
        "\n",
        "  "
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a82nfl4OKU7V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaMu9v6TPO9f"
      },
      "source": [
        "fname = ('/home/gurjeet/dbn/emnist-letters.mat')\n",
        "\n",
        "layers_nn = np.array([50, 50, 200])\n",
        "\n",
        "x1 = time.strftime('%s')\n",
        "dbn_model = DBN(layers_nn = layers_nn)\n",
        "x_train, y_train, x_test, y_test, x_trainGPU = dbn_model.load_data(fname)\n",
        "\n",
        "x2 = time.strftime('%s')\n",
        "timediff = int(x2) - int(x1)\n",
        "#print '\\nElapsed time: ', timediff, ' seconds.'\n",
        "#DN['learningtime'] = timediff\n",
        "# save final network and parameters\n",
        "#scipy.io.savemat('DN.mat', {'DN': DN})"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "AGRQyeCgjrlx",
        "outputId": "dc3d88bd-aa08-4289-c2b9-4e3943b7724d"
      },
      "source": [
        "dbn_model.train_dbn(x_trainGPU)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training layer  1 ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-220-c8cef0378bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdbn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trainGPU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-218-27efc5f80baf>\u001b[0m in \u001b[0;36mtrain_dbn\u001b[0;34m(self, batchdata)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mposvisact_GPU\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mdata_GPU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;31m#%%%%%%%% END OF POSITIVE PHASE %%%%%%%%%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mposhidstates_GPU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposhidprobs_GPU\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mposhidprobs_GPU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;31m#%%%%%%%% START NEGATIVE PHASE  %%%%%%%%%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'rand'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "IAQv1vJKT_JO",
        "outputId": "f6f001e8-c3ba-4f84-946e-3af38fd839f2"
      },
      "source": [
        "data_mb.shape\n",
        "vishid_GPU.shape"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-205-68e979eeaba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvishid_GPU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_mb' is not defined"
          ]
        }
      ]
    }
  ]
}